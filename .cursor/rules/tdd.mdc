---
description: Test-Driven Development
globs:
alwaysApply: true
---

<agent_purpose>
  Make and keep the codebase correct and malleable by practicing strict Test-Driven Development (TDD) on every change.
</agent_purpose>

<canon_tdd_workflow>
  <rules>
    - Maintain a living TEST LIST (tests/todo.md). Create it if missing.
    - Always pick exactly one item from the TEST LIST. Do not batch.
    - RED: Write a concrete, executable test that fails for that item. Run the whole test suite and confirm the new test fails.
    - GREEN: Write the minimum production code to make all tests pass. Prefer "fake it", then triangulate, then obvious implementation.
    - REFACTOR: While all tests are green, improve design of production code and tests. No behavior changes.
    - Repeat until the TEST LIST is empty.
  </rules>
  <guardrails>
    - Never write production code without a currently failing test.
    - Never write more test than needed for the chosen item.
    - After any code change, always run the full test suite.
    - If a change breaks prior tests during REFACTOR, stop and re-green before continuing.
    - Update the TEST LIST after each cycle: add discoveries, remove completed items.
  </guardrails>
</canon_tdd_workflow>

<planning_and_self_reflection>
  - Before coding each cycle, internally draft a tiny plan: chosen test, expected failure, smallest passing change, likely refactors.
  - If the “smallest passing change” feels large, decompose the TEST LIST item and take a smaller slice.
</planning_and_self_reflection>

<code_editing_rules>
  <guiding_principles>
    - One failing test at a time; keep cycles small (minutes).
    - Separate interface choices (made in tests) from internal design (shaped during refactor).
    - Prefer simple designs; duplicate tactically, then remove via refactor.
    - Keep tests readable, isolated, and fast.
  </guiding_principles>
  <commit_protocol>
    - Commit after each GREEN and after each REFACTOR.
    - Message format:
      * RED: "test: add <behavior> (failing)"
      * GREEN: "feat: pass <behavior>"
      * REFACTOR: "refactor: <scope> (no behavior change)"
  </commit_protocol>
</code_editing_rules>

<tooling_defaults>
  - If Node/TS: use Vitest or Jest; dirs: src/, tests/ or __tests__/
  - If Python: use pytest; dirs: package/, tests/
  - If Go: use `go test`; *_test.go in same package
  - If Rust: use `cargo test`; tests in tests/ or mod tests {}
  - If Java/Kotlin: use JUnit 5; src/test/java|kotlin
  - If Ruby: use RSpec; spec/
  - If none detected: propose a lightweight test stack and initialize it; then continue TDD.
</tooling_defaults>

<persistence>
  - Do not ask the human to confirm or clarify mid-cycle. Choose reasonable assumptions, proceed, and document in TEST LIST and commits.
  - Default tool budget per cycle: ≤2 project scans, ≤3 test runs pre-GREEN, ≤3 post-refactor. Prefer local reasoning over large context gathers.
</persistence>

<execution_protocol>
  1) On start or new task:
     - Load the Notion page via notion-mcp.fetch({{NOTION_TASK_URL}}).
     - Extract title, acceptance criteria, scope, constraints, links.
     - Create/refresh a spec in spec-workflow; record assumptions and constraints.
     - Create/refresh TEST LIST with concrete behaviors (happy path, boundaries, errors, integration seams).
     - Show chosen next item and the tiny plan (private notes allowed).
  2) For each cycle:
     a) RED: add one failing test; show diff; run full suite; surface failure.
     b) GREEN: implement the smallest change (prefer Serena symbol tools); show diff; run suite to green.
     c) REFACTOR: list intended refactors; apply small steps; keep green; show diffs.
     d) Update TEST LIST; commit using the protocol; update Notion status/properties via notion-mcp.update-page with links to diffs/spec.
</execution_protocol>

<non_compliance_handling>
  - If asked to implement without tests, first extend TEST LIST, then proceed with RED.
  - If a requested change spans multiple behaviors, split into separate TEST LIST items and handle one at a time.
</non_compliance_handling>
</rulefile>

<!-- Operating Procedure -->
<operating_procedure>
  1. Notion intake
     - Call notion-mcp.fetch({{NOTION_TASK_URL}}). Parse acceptance criteria → convert to TEST LIST items.
     - If database properties like Status, Assignee, Due exist, keep them up to date via update-page.
  2. Spec workflow
     - Ensure a spec exists in spec-workflow for this task; create if missing. Attach Notion URL and repo link.
     - Record decisions/assumptions; use the dashboard/tasks to track progress.
  3. Implementation (TDD)
     - Test runner per <tooling_defaults>. Initialize test harness if missing.
     - Use Serena tools (e.g., find_symbol, insert_after_symbol) for precise edits; avoid whole-file rewrites.
     - For library/API usage, append “use context7” to retrieval/generation sub-prompts to load current docs/snippets.
  4. Synchronization
     - After each GREEN/REFACTOR: commit; update TEST LIST and spec; then notion-mcp.update-page to reflect status, link to spec section and commit hash.
  5. Completion
     - All acceptance criteria satisfied and tests green.
     - Mark Notion Status = “Complete” (or equivalent) and summarize the implementation + test evidence on the page.
</operating_procedure>

<!-- Rubric (self-check before calling a task “done”) -->
<rubric>
  - Correctness (tests cover acceptance criteria; suite green)
  - Minimality (smallest change to pass)
  - Readability (tests & code clear; refactors applied)
  - Traceability (Notion ↔ Spec ↔ Commits cross-linked)
  - Safety (no leaked secrets; no destructive edits)
  - Docs (updated README/CHANGELOG if behavior is user-visible)
</rubric>

<!-- Tool Use Policy -->
<tool_use>
  - Default: local reasoning; call tools only when they advance the current TDD step.
  - notion-mcp: must call at start and at each status change.
  - spec-workflow: create/list/execute tasks to mirror progress.
  - serena: prefer symbol-level edits; avoid regex-only file surgery.
  - context7: add “use context7” whenever code gen or API behavior is involved.
</tool_use>

<!-- Output to User -->
<output_format>
  - Show: current TEST LIST excerpt (top 3 items), chosen item, tiny plan.
  - For each cycle: test diff → suite result; code diff; brief refactor notes; Notion status update summary.
  - At end: summary with links (Notion page, spec, key commits).
</output_format>
